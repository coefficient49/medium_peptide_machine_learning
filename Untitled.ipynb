{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN, GRU, LSTM, Bidirectional\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    " \n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "sys.path.append(YOUR PATH)\n",
    "import confusion_matrix_plot\n",
    "import train_validation_plots\n",
    " \n",
    "sys.path.append(YOUR PATH)\n",
    "import encode_biological_sequence\n",
    " \n",
    "# *******************************************************************\n",
    "             \n",
    "def cnn_one_layer(xtrainraw, ytrain, xtestraw, ytest, num_classes,\n",
    "                  type_of_sequence, type_of_encoding, embedding_dimension,\n",
    "                  num_epochs, batchsize,\n",
    "                  conv_window_size, pooling_size,\n",
    "                  verbosity, results_dir, class_names,\n",
    "                  xprodraw, yprod):\n",
    "     \n",
    "    model_name = 'CNN_Conv_Window_Size_' + str(conv_window_size) + '_' + type_of_encoding.title()\n",
    "     \n",
    "    vocab_size = 50  # > number of possible unique words\n",
    "    maximum_length = 0\n",
    "    if type_of_encoding == 'embedding':\n",
    "        # integer encode the peptides\n",
    "                 \n",
    "        peptides_integer_encoded_train = []\n",
    "        for pep in xtrainraw:\n",
    "            pie = encode_biological_sequence.encode_sequence(pep,\n",
    "                  type_of_sequence, type_of_encoding)\n",
    "            peptides_integer_encoded_train.append(pie)\n",
    "         \n",
    "        peptides_integer_encoded_test = []\n",
    "        for pep in xtestraw:\n",
    "            pie = encode_biological_sequence.encode_sequence(pep,\n",
    "                  type_of_sequence, type_of_encoding)\n",
    "            peptides_integer_encoded_test.append(pie)\n",
    "         \n",
    "        peptides_integer_encoded_prod = []\n",
    "        for pep in xprodraw:\n",
    "            pie = encode_biological_sequence.encode_sequence(pep,\n",
    "                  type_of_sequence, type_of_encoding)\n",
    "            peptides_integer_encoded_prod.append(pie)\n",
    "                         \n",
    "        # add padding so that all have the same length\n",
    "        max_length_train = max(len(p) for p in peptides_integer_encoded_train)\n",
    "        max_length_test = max(len(p) for p in peptides_integer_encoded_test)\n",
    "        maximum_length = max(max_length_train, max_length_test)\n",
    "        assert(vocab_size > maximum_length)\n",
    "         \n",
    "        xtrain_cnn = pad_sequences(peptides_integer_encoded_train, \n",
    "                                   maxlen=maximum_length, padding='post')\n",
    "        xtest_cnn = pad_sequences(peptides_integer_encoded_test, \n",
    "                                  maxlen=maximum_length, padding='post')\n",
    "        xprod_cnn = pad_sequences(peptides_integer_encoded_prod, \n",
    "                                  maxlen=maximum_length, padding='post')\n",
    "    else:\n",
    "        raise NameError\n",
    "                   \n",
    "    # create the cnn\n",
    "    num_output_nodes = num_classes\n",
    "    layer_1_units = 32\n",
    "    model_cnn = Sequential()\n",
    "    model_cnn.add(Embedding(vocab_size, embedding_dimension, \n",
    "                           input_length=maximum_length))\n",
    "    model_cnn.add(Conv1D(filters=layer_1_units, kernel_size=conv_window_size, activation='relu'))\n",
    "    model_cnn.add(MaxPooling1D(pool_size=pooling_size))\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(num_output_nodes, activation='softmax'))\n",
    "    # compile the model\n",
    "    model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    # summarize the model\n",
    "    print('\\n** results for simple cnn:')\n",
    "    print(model_cnn.summary())\n",
    "    # fit the model\n",
    "    h = model_cnn.fit(xtrain_cnn, ytrain, epochs=num_epochs, \n",
    "                            batch_size=batchsize,\n",
    "                            validation_data=(xtest_cnn, ytest), \n",
    "                            verbose=verbosity)\n",
    "     \n",
    "    start_epoch=10\n",
    "    # plot loss\n",
    "    plot_title = model_name.replace('_',' ') + ' Loss'\n",
    "    train_validation_plots.plot_train_validation(h.history['loss'], 'Training Loss',  \n",
    "              h.history['val_loss'], 'Validation Loss',\n",
    "              start_epoch,\n",
    "              'Epochs', 'Loss',\n",
    "              plot_title, results_dir, False)\n",
    "     \n",
    "    # plot accuracy\n",
    "    plot_title = model_name.replace('_',' ') + ' Accuracy'\n",
    "    train_validation_plots.plot_train_validation(h.history['acc'], 'Training Accuracy',  \n",
    "              h.history['val_acc'], 'Validation Accuracy',\n",
    "              start_epoch,\n",
    "              'Epochs', 'Accuracy',\n",
    "              plot_title, results_dir, False)\n",
    "   \n",
    "    y_test = np.argmax(ytest, axis=1)\n",
    "    y_test_predict = np.argmax(model_cnn.predict(xtest_cnn), axis=1)\n",
    "     \n",
    "    y_prod = np.argmax(yprod, axis=1)\n",
    "    y_prod_predict = np.argmax(model_cnn.predict(xprod_cnn), axis=1)\n",
    "        \n",
    "    # compute and print error metrics to model_name.txt\n",
    "    stdout_default = sys.stdout\n",
    "    sys.stdout = open(results_dir + model_name + '.txt','w')\n",
    "    print(model_cnn.summary(),'\\n***\\n\\n')\n",
    "    # test metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_test_predict)\n",
    "    accuracy = accuracy_score(y_test, y_test_predict, normalize=True)  # fractions\n",
    "    print('test metrics: balanced accuracy =', balanced_accuracy)\n",
    "    print('test metrics: accuracy =',accuracy)\n",
    "    print('test classification report:\\n',classification_report(y_test, y_test_predict))\n",
    "     \n",
    "    # production metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(y_prod, y_prod_predict)\n",
    "    accuracy = accuracy_score(y_prod, y_prod_predict, normalize=True)  # fractions\n",
    "    print('\\n\\nproduction metrics: balanced accuracy =', balanced_accuracy)\n",
    "    print('production metrics: accuracy =',accuracy)\n",
    "    print('production classification report:\\n',classification_report(y_prod, y_prod_predict))\n",
    "    sys.stdout = stdout_default\n",
    "        \n",
    "    # compute confusion matrices for production predictions\n",
    "    confusion_matrix_plot.cm_plot(y_prod, y_prod_predict, class_names, results_dir,\n",
    "                                  False, False, 30)  # raw\n",
    " \n",
    "    confusion_matrix_plot.cm_plot(y_prod, y_prod_predict, class_names, results_dir,\n",
    "                                  False, True, 30)  # normalized\n",
    "     \n",
    "# end of cnn_one_layer()\n",
    " \n",
    "# *******************************************************************\n",
    "             \n",
    "def cnn_one_layer_check_early_stop_cv(xcalibraw, ycalib, num_classes,\n",
    "                  type_of_sequence, type_of_encoding, embedding_dimension,\n",
    "                  num_epochs, batchsize,\n",
    "                  conv_window_size, pooling_size,\n",
    "                  verbosity, results_dir, class_names,\n",
    "                  xprodraw, yprod, stop_epochs, folds):\n",
    "     \n",
    "    model_name = 'CNN_1_Conv_Window_Size_' + str(conv_window_size) + '_' + type_of_encoding.title()\n",
    "    \n",
    "    vocab_size = 50  # > number of possible unique words\n",
    "    maximum_length = 0\n",
    "    if type_of_encoding == 'embedding':\n",
    "        # integer encode the peptides               \n",
    "        peptides_integer_encoded_calib = []\n",
    "        for pep in xcalibraw:\n",
    "            pie = encode_biological_sequence.encode_sequence(pep,\n",
    "                  type_of_sequence, type_of_encoding)\n",
    "            peptides_integer_encoded_calib.append(pie)\n",
    "         \n",
    "        peptides_integer_encoded_prod = []\n",
    "        for pep in xprodraw:\n",
    "            pie = encode_biological_sequence.encode_sequence(pep,\n",
    "                  type_of_sequence, type_of_encoding)\n",
    "            peptides_integer_encoded_prod.append(pie)\n",
    "                        \n",
    "        # add padding so that all have the same length\n",
    "        maximum_length = max(len(p) for p in peptides_integer_encoded_calib)\n",
    "        assert(vocab_size > maximum_length)\n",
    "         \n",
    "        xcalib_cnn = pad_sequences(peptides_integer_encoded_calib, \n",
    "                                   maxlen=maximum_length, padding='post')\n",
    "        xprod_cnn = pad_sequences(peptides_integer_encoded_prod, \n",
    "                                  maxlen=maximum_length, padding='post')\n",
    "    else:\n",
    "        raise NameError\n",
    "                 \n",
    "    # set up the cnn\n",
    "    metric = 'acc'\n",
    "    model_file = model_name + '.h5'\n",
    "    callbacks_list = [\n",
    "            EarlyStopping(monitor='val_loss', patience=stop_epochs),\n",
    "            ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True)]\n",
    "    num_output_nodes = num_classes\n",
    "    layer_1_units = 32\n",
    "     \n",
    "    list_cv_metrics = []\n",
    "    list_cv_losses = []\n",
    "    list_cv_epochs = []\n",
    "    cv_num = int(xcalib_cnn.shape[0]/folds)\n",
    "    for fold in range(folds):\n",
    "        # create the cnn\n",
    "        model_cnn = Sequential()\n",
    "        model_cnn.add(Embedding(vocab_size, embedding_dimension, \n",
    "                               input_length=maximum_length))\n",
    "        model_cnn.add(Conv1D(filters=layer_1_units, kernel_size=conv_window_size, \n",
    "                             activation='relu'))\n",
    "        model_cnn.add(MaxPooling1D(pool_size=pooling_size))\n",
    "        model_cnn.add(Flatten())\n",
    "        model_cnn.add(Dense(num_output_nodes, activation='softmax'))\n",
    "        # compile the model\n",
    "        model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                          metrics=[metric])\n",
    "         \n",
    "        # get train/valid\n",
    "        x_train = np.vstack((xcalib_cnn[:cv_num*fold], xcalib_cnn[cv_num*(fold+1):]))\n",
    "        y_train = np.vstack((ycalib[:cv_num*fold], ycalib[cv_num*(fold+1):]))\n",
    "         \n",
    "        x_valid = xcalib_cnn[cv_num*fold:cv_num*(fold+1)]\n",
    "        y_valid = ycalib[cv_num*fold:cv_num*(fold+1)]\n",
    "         \n",
    "        # fit the model\n",
    "        h = model_cnn.fit(x_train, y_train, \n",
    "                          epochs=num_epochs, \n",
    "                          batch_size=batchsize,\n",
    "                          validation_data=(x_valid, y_valid), \n",
    "                          callbacks=callbacks_list,\n",
    "                          verbose=verbosity)\n",
    "         \n",
    "        # collect cv stats\n",
    "        cv_loss, cv_metric = model_cnn.evaluate(x_valid, y_valid, verbose=0)\n",
    "        list_cv_metrics.append(cv_metric)\n",
    "        list_cv_losses.append(cv_loss)\n",
    "        list_cv_epochs.append(len(h.history['val_loss']))\n",
    "            \n",
    "    # redo the model with the maximum epochs from the cv and all calib data, no early stopping\n",
    "    model_cnn_final = Sequential()\n",
    "    model_cnn_final.add(Embedding(vocab_size, embedding_dimension, \n",
    "                           input_length=maximum_length))\n",
    "    model_cnn_final.add(Conv1D(filters=layer_1_units, kernel_size=conv_window_size, \n",
    "                         activation='relu'))\n",
    "    model_cnn_final.add(MaxPooling1D(pool_size=pooling_size))\n",
    "    model_cnn_final.add(Flatten())\n",
    "    model_cnn_final.add(Dense(num_output_nodes, activation='softmax'))\n",
    "    # compile the model\n",
    "    model_cnn_final.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "                            metrics=[metric])\n",
    "    # summarize the model\n",
    "    print('\\n** final results for',model_name)\n",
    "    # fit the model\n",
    "    h = model_cnn_final.fit(xcalib_cnn, ycalib, \n",
    "                      epochs=np.max(list_cv_epochs), \n",
    "                      batch_size=batchsize,\n",
    "                      verbose=verbosity)\n",
    "         \n",
    "    y_prod = np.argmax(yprod, axis=1)\n",
    "    y_prod_predict = np.argmax(model_cnn_final.predict(xprod_cnn), axis=1)\n",
    "         \n",
    "    # compute and print error metrics to model_name.txt\n",
    "    stdout_default = sys.stdout\n",
    "    sys.stdout = open(results_dir + model_name + '.txt','w')\n",
    "    print(model_cnn_final.summary(),'\\n***\\n\\n')\n",
    "    print('\\ncross validation metrics:')\n",
    "    for i in range(folds):\n",
    "        print('fold:',i+1,'\\terror:',list_cv_metrics[i],'\\tloss:',list_cv_losses[i],\n",
    "              '\\tepochs:',list_cv_epochs[i])\n",
    "         \n",
    "    print('\\nmax cross validation metric =',np.max(list_cv_metrics))\n",
    "    print('min cross validation metric =',np.min(list_cv_metrics))\n",
    "    print('mean cross validation error =',np.mean(list_cv_metrics))\n",
    "     \n",
    "    # production metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(y_prod, y_prod_predict)\n",
    "    accuracy = accuracy_score(y_prod, y_prod_predict, normalize=True)  # fractions\n",
    "    print('\\n\\nproduction metrics: balanced accuracy =', balanced_accuracy)\n",
    "    print('production metrics: accuracy =',accuracy)\n",
    "    print('production classification report:\\n',classification_report(y_prod, y_prod_predict))\n",
    "    sys.stdout = stdout_default\n",
    "     \n",
    "     \n",
    "    # compute confusion matrices for production predictions\n",
    "    confusion_matrix_plot.cm_plot(y_prod, y_prod_predict, class_names, results_dir,\n",
    "                                  False, False, 30)  # raw\n",
    " \n",
    "    confusion_matrix_plot.cm_plot(y_prod, y_prod_predict, class_names, results_dir,\n",
    "                                  False, True, 30)  # normalized\n",
    "     \n",
    "# end of cnn_one_layer_check_early_stop_cv()\n",
    " \n",
    "# *******************************************************************\n",
    " \n",
    "if __name__ == '__main__':\n",
    "     \n",
    "    code_dir = YOUR PATH\n",
    "    data_dir = code_dir + 'data/'\n",
    "     \n",
    "    df_train = pd.read_pickle(data_dir + 'df_train.pkl')\n",
    "    df_test = pd.read_pickle(data_dir + 'df_test.pkl')\n",
    "    df_production = pd.read_pickle(data_dir + 'df_production.pkl')\n",
    "     \n",
    "    number_of_classes = df_train['label_num'].value_counts().shape[0]\n",
    "    sequence_type = 'amino_acid_20'\n",
    "    encoding_type = 'embedding'\n",
    "         \n",
    "    dimension_of_embedding = 32\n",
    "     \n",
    "    epochs = 150\n",
    "    batch_size = 128\n",
    "    verbose = 0\n",
    "     \n",
    "    x_train = df_train['peptide'].values\n",
    "    y_train = to_categorical(df_train['label_num'].values)\n",
    "     \n",
    "    x_test = df_test['peptide'].values\n",
    "    y_test = to_categorical(df_test['label_num'].values)\n",
    "     \n",
    "    x_production = df_production['peptide'].values\n",
    "    y_production = to_categorical(df_production['label_num'].values)\n",
    "     \n",
    "    df_calib = pd.concat([df_train, df_test])\n",
    "    x_calib = df_calib['peptide'].values\n",
    "    y_calib = to_categorical(df_calib['label_num'].values)\n",
    "     \n",
    "    class_labels = ['Non Binder','Weak Binder','Strong Binder']\n",
    "     \n",
    "    # cnn - single layer, no regularization, no cross valid, no early stopping\n",
    "    pool_size = 3\n",
    "    for convolution_window_size in [5,7]:\n",
    "        results_directory = code_dir + 'cnn_1/conv_window_size_' + str(convolution_window_size) + '/'\n",
    "        if not Path(results_directory).is_dir():\n",
    "            os.mkdir(results_directory)\n",
    "             \n",
    "        cnn_one_layer(x_train, y_train, x_test, y_test, number_of_classes,\n",
    "                      sequence_type, encoding_type, dimension_of_embedding,\n",
    "                      epochs, batch_size, convolution_window_size, pool_size,\n",
    "                      verbose, results_directory, class_labels,\n",
    "                      x_production, y_production)\n",
    "     \n",
    "    # cnn - 1 layer, early stopping, checkpoint, cross valid, no regularization\n",
    "    cv_folds = 5\n",
    "    checkpoint_epochs_stop = 10\n",
    "    pool_size = 3\n",
    "    cnn_1_directory = code_dir + 'cnn_1_cv/'\n",
    "    if not Path(cnn_1_directory).is_dir():\n",
    "        os.mkdir(cnn_1_directory)\n",
    "    for convolution_window_size in [5,7]:\n",
    "        results_directory = cnn_1_directory + 'conv_window_size_' + str(convolution_window_size) + '/'\n",
    "        if not Path(results_directory).is_dir():\n",
    "            os.mkdir(results_directory)\n",
    "                         \n",
    "        cnn_one_layer_check_early_stop_cv(x_calib, y_calib, number_of_classes,\n",
    "                      sequence_type, encoding_type, dimension_of_embedding,\n",
    "                      epochs, batch_size, convolution_window_size, pool_size,\n",
    "                      verbose, results_directory, class_labels,\n",
    "                      x_production, y_production, checkpoint_epochs_stop, cv_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loss and accuracy plotting code:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def plot_train_validation(train_data, train_label,  \n",
    "                          valid_data, valid_label,\n",
    "                          start_num,\n",
    "                          xlabel, ylabel,\n",
    "                          title, directory, show_plot):\n",
    "     \n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots()\n",
    "     \n",
    "    plt.plot(train_data[start_num:], 'bo', label=train_label)\n",
    "    plt.plot(valid_data[start_num:], 'ro', label=valid_label)\n",
    "    ax.set_xlim(start_num, )\n",
    "    plt.xlabel(xlabel, fontsize=15)\n",
    "    plt.ylabel(ylabel, fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.grid()\n",
    "     \n",
    "    plt.savefig(directory + title + '.png')\n",
    "    plt.clf()\n",
    "    plt.close('all')\n",
    "\n",
    "#%% Confusion matrix plotting code:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "def cm_plot(y_true, y_pred, class_names, plot_dir,\n",
    "            show_plot=False, normalize=False, xangle=30):\n",
    "     \n",
    "    # set color map and precision (for normalized cm)\n",
    "    cmap=plt.cm.Blues\n",
    "    np.set_printoptions(precision=3)\n",
    "     \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = 'Normalized Confusion Matrix'\n",
    "    else:\n",
    "        title = 'Confusion Matrix'\n",
    " \n",
    "    plt.ioff()\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_names, yticklabels=class_names,\n",
    "           title=title,\n",
    "           ylabel='True Label',\n",
    "           xlabel='Predicted Label')\n",
    " \n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=xangle, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    " \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "         \n",
    "    #save plot\n",
    "    plt.savefig(plot_dir + title + '.png')\n",
    "    plt.clf()\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
